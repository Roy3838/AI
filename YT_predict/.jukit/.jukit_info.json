{"cmd": "\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\n\nimport numpy as np\nimport keras\nfrom keras import layers\nimport keras_nlp\n\nvocab_size = 10000  # Adjust based on your vocabulary size\nembedding_dim = 256\nmax_length = 100  # Adjust based on your titles' maximum length\nnum_heads = 8  # Number of attention heads in the Transformer encoder\nintermediate_dim = 512  # Dimensionality of the encoder's intermediate (feed-forward) layer\n\n# Define input layer\ninputs = keras.Input(shape=(max_length,), dtype='int64')\n\n# Token and position embedding layer\nembedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n    vocabulary_size=vocab_size,\n    sequence_length=max_length,\n    embedding_dim=embedding_dim,\n)\nx = embedding_layer(inputs)\n\n# Transformer encoder layer\nencoder = keras_nlp.layers.TransformerEncoder(\n    num_heads=num_heads,\n    intermediate_dim=intermediate_dim,\n    activation='relu',\n    dropout=0.1,\n)\nx = encoder(x)\n\n# Since we're working on a regression task, a GlobalMaxPooling1D layer is used to reduce the sequence dimension\nx = layers.GlobalMaxPooling1D()(x)\n\n# Additional dense layers for further processing\nx = layers.Dense(256, activation='relu')(x)\noutputs = layers.Dense(1, activation='linear')(x)  # Linear activation for a regression task\n\n# Compile the model\nmodel = keras.Model(inputs=inputs, outputs=outputs)\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4), loss='mean_squared_error')\n\nmodel.summary()\n\n", "cmd_opts": " --cell_id=UZ5jTpSq96 -s", "import_complete": 1, "terminal": "nvimterm"}